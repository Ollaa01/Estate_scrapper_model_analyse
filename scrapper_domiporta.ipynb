{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ostatnia strona: 36\n",
      "Liczba stron: 36\n",
      "Pobieram linki z strony 1...\n",
      "Pobieram linki z strony 2...\n",
      "Pobieram linki z strony 3...\n",
      "Pobieram linki z strony 4...\n",
      "Pobieram linki z strony 5...\n",
      "Pobieram linki z strony 6...\n",
      "Pobieram linki z strony 7...\n",
      "Pobieram linki z strony 8...\n",
      "Pobieram linki z strony 9...\n",
      "Pobieram linki z strony 10...\n",
      "Pobieram linki z strony 11...\n",
      "Pobieram linki z strony 12...\n",
      "Pobieram linki z strony 13...\n",
      "Pobieram linki z strony 14...\n",
      "Pobieram linki z strony 15...\n",
      "Pobieram linki z strony 16...\n",
      "Pobieram linki z strony 17...\n",
      "Pobieram linki z strony 18...\n",
      "Pobieram linki z strony 19...\n",
      "Pobieram linki z strony 20...\n",
      "Pobieram linki z strony 21...\n",
      "Pobieram linki z strony 22...\n",
      "Pobieram linki z strony 23...\n",
      "Pobieram linki z strony 24...\n",
      "Pobieram linki z strony 25...\n",
      "Pobieram linki z strony 26...\n",
      "Pobieram linki z strony 27...\n",
      "Pobieram linki z strony 28...\n",
      "Pobieram linki z strony 29...\n",
      "Pobieram linki z strony 30...\n",
      "Pobieram linki z strony 31...\n",
      "Pobieram linki z strony 32...\n",
      "Pobieram linki z strony 33...\n",
      "Pobieram linki z strony 34...\n",
      "Pobieram linki z strony 35...\n",
      "Pobieram linki z strony 36...\n",
      "Znaleziono 950 unikalnych linków, zapisano je w pliku Excel.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd  \n",
    "from datetime import date\n",
    "\n",
    "BASE_URL = \"https://www.domiporta.pl/mieszkanie/sprzedam?Localizations%5B0%5D.Name=Katowice\"\n",
    "DOMAIN = \"https://www.domiporta.pl\"\n",
    "data = date.today()\n",
    "\n",
    "def get_total_pages(url):\n",
    "    \"\"\"Pobiera liczbę stron z głównej strony Domiporta.\"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}  \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Błąd: {response.status_code}\")\n",
    "        return 0\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    pagination = soup.find_all(\"a\", href=True)\n",
    "    max_page = 0\n",
    "    \n",
    "    for tag in pagination:\n",
    "        try:\n",
    "            if \"PageNumber=\" in tag['href']: \n",
    "                page_number = int(tag.text.strip())\n",
    "                max_page = max(max_page, page_number)\n",
    "        except (ValueError, KeyError):\n",
    "            continue\n",
    "    \n",
    "    if max_page > 0:\n",
    "        print(f\"Ostatnia strona: {max_page}\")\n",
    "        return max_page\n",
    "    else:\n",
    "        print(\"Nie udało się znaleźć numeru ostatniej strony.\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_page_links(url, domain):\n",
    "    \"\"\"Pobiera linki z danej strony Domiporta.\"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"} \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Błąd: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    offer_links = soup.find_all(\"a\", href=True)\n",
    "    filtered_links = []\n",
    "    \n",
    "    for link in offer_links:\n",
    "        href = link['href']\n",
    "        full_url = domain + href if href.startswith(\"/\") else href\n",
    "        if full_url.startswith(\"https://www.domiporta.pl/nieruchomosci/sprzedam-\"):\n",
    "            filtered_links.append(full_url)\n",
    "    \n",
    "    return list(set(filtered_links))\n",
    "\n",
    "\n",
    "def save_links_to_excel(links, filename):\n",
    "    df = pd.DataFrame(links, columns=[\"Link\"])  \n",
    "    df.to_excel(filename, index=False, engine='openpyxl')  \n",
    "\n",
    "\n",
    "def scrape_domiporta():\n",
    "    total_pages = get_total_pages(BASE_URL)\n",
    "    if total_pages == 0:\n",
    "        print(\"Nie udało się pobrać liczby stron.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Liczba stron: {total_pages}\")\n",
    "    \n",
    "    all_links = []\n",
    "    \n",
    "    for page in range(1, total_pages + 1):\n",
    "        url = f\"{BASE_URL}&PageNumber={page}\"\n",
    "        print(f\"Pobieram linki z strony {page}...\")\n",
    "        links = get_page_links(url, DOMAIN)\n",
    "        all_links.extend(links)\n",
    "    \n",
    "    all_links = list(set(all_links))\n",
    "    \n",
    "    save_links_to_excel(all_links, rf\"domiporta\\domiporta_links_{data}.xlsx\")\n",
    "    \n",
    "    print(f\"Znaleziono {len(all_links)} unikalnych linków, zapisano je w pliku Excel.\")\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_domiporta()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "urls = pd.read_excel(rf'domiporta\\domiporta_links_{data}.xlsx')\n",
    "urls_list = urls['Link'].tolist()\n",
    "random_urls = random.sample(urls_list, 10) if len(urls_list) >= 50 else urls_list\n",
    "def safe_find_text(element):\n",
    "    return element.get_text(strip=True) if element else None\n",
    "\n",
    "def save_page_html(url):\n",
    "    \"\"\"Pobiera stronę i zapisuje jej HTML do słownika z danymi.\"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"} \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Błąd: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    features_section = soup.find('section', class_='detials__section features')\n",
    "    feature_containers = features_section.find_all('div', class_='features__container') if features_section else []\n",
    "\n",
    "    features_data = {}\n",
    "\n",
    "    for container in feature_containers:\n",
    "        if container.find('ul'):\n",
    "            features_list = container.find_all('li')\n",
    "            for feature in features_list:\n",
    "                name = safe_find_text(feature.find('span', class_='features__item_name'))\n",
    "                value = safe_find_text(feature.find('span', class_='features__item_value'))\n",
    "                if name and value:\n",
    "                    features_data[name] = value\n",
    "        elif container.find('dl'):\n",
    "            feature_details = container.find_all('dl')\n",
    "            for detail in feature_details:\n",
    "                name = safe_find_text(detail.find('dt', class_='features__item_name'))\n",
    "                value = safe_find_text(detail.find('dd', class_='features__item_value'))\n",
    "                if name and value:\n",
    "                    features_data[name] = value\n",
    "    \n",
    "    contact_section = soup.find('div', class_='contact__data_container details-databox')\n",
    "    advertiser_name = safe_find_text(contact_section.find('p', class_='details-databox__name')) if contact_section else None\n",
    "    advertiser_address = safe_find_text(contact_section.find('span')) if contact_section else None\n",
    "    \n",
    "    phone_elements = contact_section.find_all('div', class_='agent__phone') if contact_section else []\n",
    "    phone_numbers = [phone['data-tel'] for phone in phone_elements if phone.get('data-tel')]\n",
    "\n",
    "    property_info = {\n",
    "        'Cena': safe_find_text(soup.find('span', {'itemprop': 'price'})),\n",
    "        'Cena za m²': safe_find_text(soup.find('span', class_='summary__subtitle--price')),\n",
    "        'Lokalizacja': safe_find_text(soup.find('span', itemprop='address')),\n",
    "        'Powierzchnia': safe_find_text(soup.find('li', string=lambda x: x and 'Powierzchnia całkowita' in x)),\n",
    "        'Liczba pokoi': safe_find_text(soup.find('li', string=lambda x: x and 'Liczba pokoi' in x)),\n",
    "        'Piętro': safe_find_text(soup.find('li', string=lambda x: x and 'Piętro' in x)),\n",
    "        'Typ budynku': safe_find_text(soup.find('li', string=lambda x: x and 'Typ budynku' in x)),\n",
    "        'Rok budowy': safe_find_text(soup.find('li', string=lambda x: x and 'Rok budowy' in x)),\n",
    "        'Materiał': safe_find_text(soup.find('li', string=lambda x: x and 'Materiał' in x)),\n",
    "        'Ogłoszeniodawca': advertiser_name,\n",
    "        'Adres ogłoszeniodawcy': advertiser_address,\n",
    "        'Numery telefonów': ', '.join(phone_numbers),\n",
    "        'URL': url\n",
    "    }\n",
    "    \n",
    "    property_info.update(features_data)\n",
    "\n",
    "    return property_info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scrape_multiple_urls(urls):\n",
    "    all_data = []\n",
    "\n",
    "    for url in urls:\n",
    "        data = save_page_html(url)\n",
    "        if data:\n",
    "            all_data.append(data)\n",
    "        else:\n",
    "            print(f\"Nie udało się przetworzyć URL: {url}\")\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = scrape_multiple_urls(urls_list)\n",
    "data = date.today()\n",
    "\n",
    "df.to_clipboard()\n",
    "data = date.today()\n",
    "df.to_excel(rf'domiporta\\domiporta_data_{data}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(rf'domiporta\\domiporta_data_{data}.xlsx')\n",
    "df = df[['Cena', 'Cena za m2', 'Lokalizacja', 'Powierzchnia całkowita', 'Liczba pokoi', 'Piętro', 'Typ budynku', 'Rok budowy', 'Materiał', 'Ogłoszeniodawca', 'URL', 'Kategoria', 'Liczba pięter w budynku', 'Forma własności',\t'Informacje dodatkowe:', 'Czynsz administracyjny', 'Powierzchnia piwnicy', 'Dostępne od']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = df.dropna(subset=['Lokalizacja'])\n",
    "\n",
    "dzielnice = [\n",
    "    'Śródmieście', 'Koszutka', 'Bogucice', 'Os. Paderewskiego - Muchowiec', \n",
    "    'Załęże', 'Osiedle Wincentego Witosa', 'Osiedle Tysiąclecia', 'Dąb', \n",
    "    'Wełnowiec-Józefowiec', 'Ligota-Panewniki', 'Brynów-Osiedle Zgrzebnioka', \n",
    "    'Załęska Hałda-Brynów', 'Zawodzie', 'Dąbrówka Mała', 'Szopienice-Burowiec', \n",
    "    'Janów-Nikiszowiec', 'Giszowiec', 'Murcki', 'Piotrowice-Ochojec', \n",
    "    'Zarzecze', 'Kostuchna', 'Podlesie', 'Centrum', 'Józefowiec', 'Brynów', 'Ligota', 'Piotrowice', 'Nikiszowiec', 'Ochojec', 'Szopienice'\n",
    "]\n",
    "\n",
    "dzielnice_set = set(dzielnice)\n",
    "\n",
    "def process_lokalizacja(lokalizacja):\n",
    "    parts = [part.strip() for part in lokalizacja.split(',')]\n",
    "\n",
    "    if len(parts) < 2 or parts[0].lower() != 'śląskie' or parts[1].lower() != 'katowice':\n",
    "        return None  \n",
    "    \n",
    "    wojewodztwo = parts[0]\n",
    "    miasto = parts[1]\n",
    "    dzielnica = \"brak informacji\"\n",
    "    ulica = \"brak informacji\"\n",
    "    \n",
    "    if len(parts) == 2:\n",
    "        return wojewodztwo, miasto, dzielnica, ulica\n",
    "\n",
    "    elif len(parts) == 3:\n",
    "        dodatkowy = parts[2]\n",
    "        if dodatkowy in dzielnice_set:\n",
    "            dzielnica = dodatkowy\n",
    "        else:\n",
    "            ulica = dodatkowy\n",
    "        return wojewodztwo, miasto, dzielnica, ulica\n",
    "    \n",
    "    elif len(parts) >= 4:\n",
    "        potencjalna_dzielnica = parts[2]\n",
    "        potencjalna_ulica = parts[3]\n",
    "        \n",
    "        if potencjalna_dzielnica in dzielnice_set:\n",
    "            dzielnica = potencjalna_dzielnica\n",
    "        else:\n",
    "            dzielnica = \"brak informacji\"\n",
    "        \n",
    "        if potencjalna_ulica:\n",
    "            ulica = potencjalna_ulica\n",
    "        return wojewodztwo, miasto, dzielnica, ulica\n",
    "\n",
    "    else:\n",
    "        potencjalna_dzielnica = parts[2]\n",
    "        potencjalna_ulica = parts[3]\n",
    "        \n",
    "        if potencjalna_dzielnica in dzielnice_set:\n",
    "            dzielnica = potencjalna_dzielnica\n",
    "        else:\n",
    "            dzielnica = \"brak informacji\"\n",
    "        \n",
    "        if potencjalna_ulica:\n",
    "            ulica = potencjalna_ulica\n",
    "        return wojewodztwo, miasto, dzielnica, ulica\n",
    "\n",
    "df[['województwo', 'miasto', 'dzielnica', 'ulica']] = df['Lokalizacja'].apply(process_lokalizacja).apply(pd.Series)\n",
    "df = df.dropna(subset=['województwo'])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_convert(column, to_remove):\n",
    "    return pd.to_numeric(\n",
    "        df[column]\n",
    "        .str.replace(to_remove, '', regex=True)  \n",
    "        .str.extract(r'(\\d{1,3}(?:[\\s,]?\\d{3})*(?:[.,]\\d+)?)')[0]  \n",
    "        .str.replace(r'[^\\d,]', '', regex=True)  \n",
    "        .str.replace(r',', '.', regex=True) \n",
    "        .str.replace(r'\\s+', '', regex=True)\n",
    "    )\n",
    "\n",
    "\n",
    "df['Cena'] = clean_and_convert('Cena', 'zł')\n",
    "df['Cena za m2'] = clean_and_convert('Cena za m2', 'zł/m2')\n",
    "df['Powierzchnia całkowita'] = clean_and_convert('Powierzchnia całkowita', 'm2')\n",
    "df['Czynsz administracyjny'] = clean_and_convert('Czynsz administracyjny', 'zł')\n",
    "df['Powierzchnia piwnicy'] = clean_and_convert('Powierzchnia piwnicy', 'm2')\n",
    "df['Piętro'] = pd.to_numeric(df['Piętro'].str.replace('Parter', '0', regex=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Informacje dodatkowe'] = df['Informacje dodatkowe:'].str.split(', ')\n",
    "features = set(sum([x for x in df['Informacje dodatkowe'] if isinstance(x, list)], []))\n",
    "for feature in features:\n",
    "    df[feature] = df['Informacje dodatkowe'].apply(lambda x: 1 if isinstance(x, list) and feature in x else 0)\n",
    "df.drop(columns=['Informacje dodatkowe'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_groups = {\n",
    "    'garaż_łącznie': 'garaż',  \n",
    "    'ogrzewanie_łącznie': 'ogrzewanie',  \n",
    "    'balkon_łącznie': ['taras', 'balkon'],  \n",
    "    'miejsce_prakingowe_łącznie': ['miejsce parkingowe', 'parking_strzeżony']\n",
    "}\n",
    "\n",
    "for new_feature, group in prefix_groups.items():\n",
    "    if isinstance(group, str):\n",
    "        group_columns = [col for col in df.columns if col.lower().startswith(group.lower())]\n",
    "    else:  \n",
    "        group_columns = [col for col in group if col in df.columns]\n",
    "\n",
    "    if group_columns:\n",
    "        df[new_feature] = df[group_columns].max(axis=1)\n",
    "        df.drop(columns=group_columns, inplace=True)  \n",
    "\n",
    "threshold = 0.05 * len(df)\n",
    "low_info_columns = [\n",
    "    col for col in df.select_dtypes(include=['number']).columns\n",
    "    if df[col].sum() < threshold\n",
    "]\n",
    "df.drop(columns=low_info_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_excel(rf'C:\\Users\\PC\\Desktop\\scrapping_danych\\domiporta\\domiporta_data_preprocess.xlsx', index=False)\n",
    "df.to_excel(rf'domiporta\\data_archive\\domiporta_data_preprocess_{data}.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
